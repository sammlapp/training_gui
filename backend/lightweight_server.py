#!/usr/bin/env python3
"""
Lightweight HTTP server for bioacoustics app backend.
This version excludes ML dependencies and uses lightweight libraries.
"""

import sys
import json
import argparse
import asyncio
import os
import tempfile
import logging
import base64
import subprocess
import threading
import tarfile
import glob
import platform
import yaml
from pathlib import Path
from aiohttp import web, web_request
from aiohttp_cors import setup as cors_setup, ResourceOptions
import pandas as pd
import numpy as np
import librosa
import scipy.signal
from PIL import Image
import soundfile as sf
from io import BytesIO

from scripts import scan_folder
from scripts import get_sample_detections
from scripts import load_scores
from scripts import clip_extraction

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Google Drive file ID for PyTorch environment
PYTORCH_ENV_FILE_ID = "1rsJjnCWjkiMDPimwg11QKsI-tOS7To8O"


def is_process_alive(pid):
    """
    Check if a process with given PID is still running.
    Cross-platform implementation for Windows, macOS, and Linux.

    Args:
        pid: Process ID to check

    Returns:
        True if process is alive, False otherwise
    """
    if pid is None or pid <= 0:
        return False

    system = platform.system()

    try:
        if system == "Windows":
            # On Windows, use tasklist command to check if process exists
            import ctypes

            kernel32 = ctypes.windll.kernel32
            PROCESS_QUERY_INFORMATION = 0x0400
            handle = kernel32.OpenProcess(PROCESS_QUERY_INFORMATION, 0, pid)
            if handle:
                kernel32.CloseHandle(handle)
                return True
            return False
        else:
            # On Unix-like systems (Linux, macOS), use os.kill with signal 0
            # Signal 0 doesn't actually send a signal, just checks if process exists
            os.kill(pid, 0)
            return True
    except (OSError, AttributeError, Exception):
        # OSError raised if process doesn't exist
        # AttributeError if ctypes not available
        return False


async def monitor_parent_process(parent_pid, shutdown_callback, check_interval=2.0):
    """
    Async task that monitors the parent process and triggers shutdown if parent dies.

    Args:
        parent_pid: PID of the parent process to monitor
        shutdown_callback: Async function to call when parent process dies
        check_interval: How often to check (in seconds)
    """
    logger.info(f"Starting parent process monitor (parent PID: {parent_pid})")
    logger.info(f"Checking parent process every {check_interval} seconds")

    check_count = 0
    while True:
        await asyncio.sleep(check_interval)
        check_count += 1

        is_alive = is_process_alive(parent_pid)
        if check_count <= 3:  # Log first few checks for debugging
            logger.info(
                f"Parent process check #{check_count}: PID {parent_pid} alive={is_alive}"
            )

        if not is_alive:
            logger.warning(f"Parent process (PID {parent_pid}) is no longer alive!")
            logger.info("Initiating graceful shutdown...")
            try:
                await shutdown_callback()
            except Exception as e:
                logger.error(f"Error during shutdown callback: {e}")
            logger.info("Shutdown callback completed, exiting monitor")
            break


def get_last_error_from_log(log_file_path, max_lines=10):
    """
    Read the last few lines from a log file to extract error information.
    Returns a brief error summary for display in the task panel.
    """
    if not log_file_path or not os.path.exists(log_file_path):
        return None

    try:
        with open(log_file_path, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()

        if not lines:
            return None

        # Get last few lines to look for errors
        last_lines = lines[-max_lines:] if len(lines) > max_lines else lines

        # Look for common error patterns
        error_keywords = [
            "ERROR",
            "Error",
            "error",
            "FAILED",
            "Failed",
            "failed",
            "Exception",
            "Traceback",
            "RuntimeError",
            "ValueError",
            "ImportError",
        ]

        error_lines = []
        for line in last_lines:
            line = line.strip()
            if any(keyword in line for keyword in error_keywords):
                error_lines.append(line)

        if error_lines:
            # Return the last error line, truncated if too long
            last_error = error_lines[-1]
            if len(last_error) > 150:
                last_error = last_error[:147] + "..."
            return last_error
        else:
            # If no explicit error found, return last non-empty line
            for line in reversed(last_lines):
                line = line.strip()
                if line and len(line) > 10:  # Skip very short lines
                    if len(line) > 150:
                        line = line[:147] + "..."
                    return line

    except Exception as e:
        logger.warning(f"Could not read log file {log_file_path}: {e}")
        return None

    return None


# Config Management Functions
def save_inference_config(config_data, output_path):
    """Save inference configuration to JSON file"""
    try:
        with open(output_path, "w") as f:
            json.dump(config_data, f, indent=2)
        return {"status": "success", "config_path": output_path}
    except Exception as e:
        return {"status": "error", "error": str(e)}


def load_inference_config(config_path):
    """Load inference configuration from JSON file"""
    try:
        with open(config_path, "r") as f:
            config = json.load(f)
        return {"status": "success", "config": config}
    except Exception as e:
        return {"status": "error", "error": str(e)}


def validate_audio_files(file_list):
    """Validate that audio files exist"""
    valid_extensions = {".wav", ".mp3", ".flac", ".ogg", ".m4a", ".aac"}
    results = {"valid_files": [], "missing_files": [], "invalid_extensions": []}

    for file_path in file_list:
        if not os.path.exists(file_path):
            results["missing_files"].append(file_path)
        elif Path(file_path).suffix.lower() not in valid_extensions:
            results["invalid_extensions"].append(file_path)
        else:
            results["valid_files"].append(file_path)

    return results


# Environment Management Functions
def get_default_env_path():
    """Get the default environment path in system-specific cache directory"""
    cache_dir = get_default_env_cache_dir()
    env_path = os.path.join(cache_dir, "envs/dipper_pytorch_env")
    return env_path
    # # Fallback to local directory # No fallback!
    # return os.path.join(os.path.expanduser("~"), ".dipper", "env")


def get_default_env_archive_path():
    """Get the default environment archive path in system-specific cache directory"""
    cache_dir = get_default_env_cache_dir()
    env_path = os.path.join(cache_dir, "archives/dipper_pytorch_env.tar.gz")
    return env_path


def get_default_env_cache_dir():
    """Get the default environment caching folder in system-specific cache directory"""
    import platformdirs

    return platformdirs.user_cache_dir("Dipper", "BioacousticsApp")


def download_environment_from_gdrive():
    """Download PyTorch environment from Google Drive to cache directory"""
    try:
        import gdown

        archive_path = get_default_env_archive_path()
        logger.info(f"Downloading PyTorch environment to {archive_path}...")
        os.makedirs(os.path.dirname(archive_path), exist_ok=True)

        # Download using gdown with file ID
        url = f"https://drive.google.com/uc?id={PYTORCH_ENV_FILE_ID}"
        gdown.download(url, archive_path, quiet=False)

        logger.info(f"Download complete: {archive_path}")
        return {"status": "success", "archive_path": archive_path}

    except Exception as e:
        logger.error(f"Error downloading environment: {e}")
        return {"status": "error", "error": str(e)}


def check_environment(env_path):
    """Check if conda-pack environment exists and is valid"""
    try:
        python_path = os.path.join(env_path, "bin", "python")
        if os.name == "nt":  # Windows
            python_path = os.path.join(env_path, "python.exe")

        if not os.path.exists(python_path):
            return {"status": "missing", "python_path": python_path}

        # Try to run a simple Python command
        result = subprocess.run(
            [python_path, "--version"], capture_output=True, text=True
        )
        if result.returncode == 0:
            return {
                "status": "ready",
                "python_path": python_path,
                "version": result.stdout.strip(),
            }
        else:
            return {
                "status": "broken",
                "error": f"Python check failed: {result.stderr}",
            }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def extract_environment(archive_path, extract_dir):
    """Extract conda-pack environment from tar.gz archive"""
    try:
        logger.info(f"Extracting environment from {archive_path} to {extract_dir}")

        # Create extraction directory
        os.makedirs(extract_dir, exist_ok=True)

        # Extract the tar.gz file
        with tarfile.open(archive_path, "r:gz") as tar:
            tar.extractall(path=extract_dir)

        # Check if extraction was successful
        env_check = check_environment(extract_dir)
        if env_check["status"] in ["ready", "missing"]:
            return {"status": "success", "env_path": extract_dir}
        else:
            return {
                "status": "error",
                "error": f"Environment extraction failed: {env_check.get('error', 'Unknown error')}",
            }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def resolve_path(path_str, base_dir=None):
    """Resolve relative or absolute paths consistently"""
    if os.path.isabs(path_str):
        # Already absolute, use as-is
        return path_str
    else:
        # Relative path - resolve relative to base_dir or current working directory
        if base_dir:
            return os.path.join(base_dir, path_str)
        else:
            return os.path.abspath(path_str)


def setup_environment(env_dir=None):
    """Set up environment - extract if needed, check if ready

    Args:
        env_dir: Environment directory if using a custom Python environment
            if None, will use the built-in Dipper environment in application cache directory
            (downloading and/or extracting .tar.gz if needed)

    Returns:
        dict: Status and details about the environment setup
    """
    try:
        # If user provided custom environment path, use that environment
        if env_dir is not None:
            # Resolve custom path
            env_dir = resolve_path(env_dir)
            logger.info(f"Using custom environment path: {env_dir}")

            env_check = check_environment(env_dir)

            if env_check["status"] == "ready":
                return {
                    "status": "ready",
                    "python_path": env_check["python_path"],
                    "message": "Environment already ready",
                }
            else:
                return {
                    "status": "error",
                    "error": f"User-specified custom environment {env_dir} could not be used: {env_check.get('error', 'Unknown error')}",
                }

        # Use the built-in Dipper environment, downloading and/or extracting .tar.gz if needed

        # first get the default environment path for this operating system
        env_dir = get_default_env_path()
        logger.info(f"Using default environment path: {env_dir}")

        # check if environment already exists
        env_check = check_environment(env_dir)

        if env_check["status"] == "ready":
            return {
                "status": "ready",
                "python_path": env_check["python_path"],
                "message": "Environment already ready",
            }

        # Environment is not yet ready
        # we may need to download it then extract it
        # or just extract it if we already have the archive file
        # in the expected location

        # Check cache directory for archive
        archive_path = get_default_env_archive_path()

        if os.path.exists(archive_path):
            logger.info(f"Using cached archive: {archive_path}")
        else:
            # Download from Google Drive
            logger.info("Archive not found in cache, downloading from Google Drive...")
            download_result = download_environment_from_gdrive()

            if download_result["status"] == "success":
                logger.info(f"Downloaded archive to: {archive_path}")
            else:
                return {
                    "status": "error",
                    "error": f"Failed to download environment: {download_result.get('error', 'Unknown error')}",
                }

        # Extract environment
        if not os.path.exists(archive_path):
            return {
                "status": "error",
                "error": f"Archive not found at {archive_path}",
            }

        # Extract the environment from the cached archive file
        logger.info(f"Extracting environment from {archive_path} to {env_dir}")
        extract_result = extract_environment(archive_path, env_dir)

        if extract_result["status"] == "success":
            # Check again after extraction
            final_check = check_environment(env_dir)
            if final_check["status"] == "ready":
                return {
                    "status": "ready",
                    "python_path": final_check["python_path"],
                    "message": "Environment extracted and ready",
                }
            else:
                return {
                    "status": "error",
                    "error": f"Environment setup failed: {final_check.get('error', 'Unknown error')}",
                }
        else:
            return extract_result

    except Exception as e:
        return {"status": "error", "error": str(e)}


# Process Management Functions
def start_inference_process(job_id, config_path, env_python_path):
    """Start inference process in background and return immediately"""
    try:
        # Resolve paths to absolute paths
        config_path = resolve_path(config_path)
        env_python_path = resolve_path(env_python_path)

        logger.info(f"Starting inference job {job_id} with config: {config_path}")
        logger.info(f"Using Python environment: {env_python_path}")

        # Verify environment exists
        if not os.path.exists(env_python_path):
            return {
                "status": "error",
                "error": f"Python environment not found: {env_python_path}",
            }

        # Verify config file exists
        if not os.path.exists(config_path):
            return {"status": "error", "error": f"Config file not found: {config_path}"}

        # Load config to get log file path and job folder
        log_file_path = None
        job_folder = None
        try:
            import json

            with open(config_path, "r") as f:
                config_data = json.load(f)
                log_file_path = config_data.get("log_file_path")
                job_folder = config_data.get("job_folder")
        except Exception as e:
            logger.warning(f"Could not read config data: {e}")

        # Run inference.py with the specified Python environment
        inference_script = os.path.join(
            os.path.dirname(__file__), "scripts", "inference.py"
        )
        cmd = [env_python_path, inference_script, "--config", config_path]

        logger.info(f"Running command: {' '.join(cmd)}")
        if log_file_path:
            logger.info(f"Redirecting output to: {log_file_path}")

        # Prepare output redirection
        if log_file_path:
            # Ensure the directory exists
            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
            # Open log file for writing
            log_file = open(log_file_path, "w")
            stdout_target = log_file
            stderr_target = (
                subprocess.STDOUT
            )  # Redirect stderr to stdout (which goes to log file)
        else:
            # Fallback to PIPE if no log file specified
            stdout_target = subprocess.PIPE
            stderr_target = subprocess.PIPE

        # Start the process (non-blocking)
        process = subprocess.Popen(
            cmd,
            stdout=stdout_target,
            stderr=stderr_target,
            text=True,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )

        # Store log file handle with process if we opened one
        if log_file_path:
            process._log_file = log_file

        return {
            "status": "started",
            "job_id": job_id,
            "process": process,
            "system_pid": process.pid,
            "command": " ".join(cmd),
            "message": "Inference process started successfully",
            "log_file_path": log_file_path,
            "job_folder": job_folder,
        }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def check_inference_status(process, job_info=None):
    """Check status of running inference process"""
    try:
        if process is None:
            return {"status": "error", "error": "No process to check"}

        # Check if process is still running
        return_code = process.poll()

        if return_code is None:
            status_response = {
                "status": "running",
                "message": "Inference process is initializing",
            }

            # Try to read detailed status from .status file
            if job_info and "job_folder" in job_info:
                status_file = os.path.join(job_info["job_folder"], ".status")
                logger.info(
                    f"[check_inference_status] Checking for status file: {status_file}"
                )
                if os.path.exists(status_file):
                    try:
                        with open(status_file, "r") as f:
                            status_data = json.load(f)
                            logger.info(
                                f"[check_inference_status] Read status file data: {status_data}"
                            )
                            # Merge status file data into response
                            if "stage" in status_data:
                                status_response["stage"] = status_data["stage"]
                            if "progress" in status_data:
                                status_response["progress"] = status_data["progress"]
                            if "message" in status_data:
                                status_response["message"] = status_data["message"]
                            if "metadata" in status_data:
                                status_response["metadata"] = status_data["metadata"]
                            logger.info(
                                f"[check_inference_status] Merged status response: {status_response}"
                            )
                    except Exception as e:
                        logger.warning(
                            f"[check_inference_status] Could not read status file: {e}"
                        )
                else:
                    logger.debug(
                        f"[check_inference_status] Status file does not exist yet: {status_file}"
                    )

            return status_response
        else:
            # Process has completed
            # Close log file if it was opened
            if hasattr(process, "_log_file"):
                try:
                    process._log_file.close()
                except:
                    pass

            # Get output - may be None if redirected to file
            stdout, stderr = process.communicate()

            logger.info(f"Inference process completed with exit code: {return_code}")
            if stdout:
                logger.info(f"Stdout: {stdout[:500]}...")  # Log first 500 chars
            if stderr:
                logger.error(f"Stderr: {stderr[:500]}...")  # Log first 500 chars

            # Check if job was cancelled before checking exit code
            if job_info and job_info.get("status") == "cancelled":
                return {
                    "status": "cancelled",
                    "message": "Inference was cancelled by user",
                    "exit_code": return_code,
                }
            elif return_code == 0:
                return {
                    "status": "completed",
                    "message": "Inference completed successfully",
                    "output": stdout or "Output redirected to log file",
                    "stdout": stdout or "Output redirected to log file",
                    "stderr": stderr if stderr else "",
                    "exit_code": return_code,
                }
            else:
                # Try to get the actual error from log file
                error_message = f"Inference failed with exit code {return_code}"
                log_file_path = None

                # Try to get log file path from job_info
                if job_info and "log_file_path" in job_info:
                    log_file_path = job_info["log_file_path"]

                # Try to get actual error from log file
                if log_file_path:
                    last_error = get_last_error_from_log(log_file_path)
                    if last_error:
                        error_message = (
                            f"{last_error} (see log file for full error message)"
                        )

                return {
                    "status": "failed",
                    "error": error_message,
                    "exit_code": return_code,
                    "stdout": stdout or "Output redirected to log file",
                    "stderr": stderr or "Error output redirected to log file",
                }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def start_training_process(job_id, config_path, env_python_path):
    """Start training process in background and return immediately"""
    try:
        # Resolve paths to absolute paths
        config_path = resolve_path(config_path)
        env_python_path = resolve_path(env_python_path)

        logger.info(f"Starting training job {job_id} with config: {config_path}")
        logger.info(f"Using Python environment: {env_python_path}")

        # Verify environment exists
        if not os.path.exists(env_python_path):
            return {
                "status": "error",
                "error": f"Python environment not found: {env_python_path}",
            }

        # Verify config file exists
        if not os.path.exists(config_path):
            return {"status": "error", "error": f"Config file not found: {config_path}"}

        # Load config to get log file path and job folder
        log_file_path = None
        job_folder = None
        try:
            import json

            with open(config_path, "r") as f:
                config_data = json.load(f)
                log_file_path = config_data.get("log_file_path")
                job_folder = config_data.get("job_folder")
        except Exception as e:
            logger.warning(f"Could not read config data: {e}")

        # Run train_model.py with the specified Python environment
        training_script = os.path.join(
            os.path.dirname(__file__), "scripts", "train_model.py"
        )
        cmd = [env_python_path, training_script, "--config", config_path]

        logger.info(f"Running command: {' '.join(cmd)}")
        if log_file_path:
            logger.info(f"Redirecting output to: {log_file_path}")

        # Prepare output redirection
        if log_file_path:
            # Ensure the directory exists
            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
            # Open log file for writing
            log_file = open(log_file_path, "w")
            stdout_target = log_file
            stderr_target = (
                subprocess.STDOUT
            )  # Redirect stderr to stdout (which goes to log file)
        else:
            # Fallback to PIPE if no log file specified
            stdout_target = subprocess.PIPE
            stderr_target = subprocess.PIPE

        # Start the process (non-blocking)
        process = subprocess.Popen(
            cmd,
            stdout=stdout_target,
            stderr=stderr_target,
            text=True,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )

        # Store log file handle with process if we opened one
        if log_file_path:
            process._log_file = log_file

        return {
            "status": "started",
            "job_id": job_id,
            "process": process,
            "system_pid": process.pid,
            "command": " ".join(cmd),
            "message": "Training process started successfully",
            "log_file_path": log_file_path,
            "job_folder": job_folder,
        }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def start_extraction_process(job_id, config_path, env_python_path):
    """Start extraction process in background and return immediately"""
    try:
        # Resolve paths to absolute paths
        config_path = resolve_path(config_path)
        env_python_path = resolve_path(env_python_path)

        logger.info(f"Starting extraction job {job_id} with config: {config_path}")
        logger.info(f"Using Python environment: {env_python_path}")

        # Verify environment exists
        if not os.path.exists(env_python_path):
            return {
                "status": "error",
                "error": f"Python environment not found: {env_python_path}",
            }

        # Verify config file exists
        if not os.path.exists(config_path):
            return {"status": "error", "error": f"Config file not found: {config_path}"}

        # Load config to get log file path and job folder
        log_file_path = None
        job_folder = None
        try:
            import json

            with open(config_path, "r") as f:
                config_data = json.load(f)
                log_file_path = config_data.get("log_file_path")
                job_folder = config_data.get("job_folder")
        except Exception as e:
            logger.warning(f"Could not read config data: {e}")

        # Run clip_extraction.py with the specified Python environment
        extraction_script = os.path.join(
            os.path.dirname(__file__), "scripts", "clip_extraction.py"
        )
        cmd = [env_python_path, extraction_script, config_path]

        logger.info(f"Running command: {' '.join(cmd)}")
        if log_file_path:
            logger.info(f"Redirecting output to: {log_file_path}")

        # Prepare output redirection
        if log_file_path:
            # Ensure the directory exists
            os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
            # Open log file for writing
            log_file = open(log_file_path, "w")
            stdout_target = log_file
            stderr_target = (
                subprocess.STDOUT
            )  # Redirect stderr to stdout (which goes to log file)
        else:
            # Fallback to PIPE if no log file specified
            stdout_target = subprocess.PIPE
            stderr_target = subprocess.PIPE

        # Start the process (non-blocking)
        process = subprocess.Popen(
            cmd,
            stdout=stdout_target,
            stderr=stderr_target,
            text=True,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )

        # Store log file handle with process if we opened one
        if log_file_path:
            process._log_file = log_file

        return {
            "status": "started",
            "job_id": job_id,
            "process": process,
            "system_pid": process.pid,
            "command": " ".join(cmd),
            "message": "Extraction process started successfully",
            "log_file_path": log_file_path,
            "job_folder": job_folder,
        }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def check_training_status(process, job_info=None):
    """Check status of running training process"""
    try:
        if process is None:
            return {"status": "error", "error": "No process to check"}

        # Check if process is still running
        return_code = process.poll()

        if return_code is None:
            # Process is still running
            status_response = {
                "status": "running",
                "message": "Training process is still running",
            }

            # Try to read detailed status from .status file
            if job_info and "job_folder" in job_info:
                status_file = os.path.join(job_info["job_folder"], ".status")
                if os.path.exists(status_file):
                    try:
                        with open(status_file, "r") as f:
                            status_data = json.load(f)
                            # Merge status file data into response
                            if "stage" in status_data:
                                status_response["stage"] = status_data["stage"]
                            if "progress" in status_data:
                                status_response["progress"] = status_data["progress"]
                            if "message" in status_data:
                                status_response["message"] = status_data["message"]
                            if "metadata" in status_data:
                                status_response["metadata"] = status_data["metadata"]
                    except Exception as e:
                        logger.debug(f"Could not read status file: {e}")

            return status_response
        else:
            # Process has completed
            # Close log file if it was opened
            if hasattr(process, "_log_file"):
                try:
                    process._log_file.close()
                except:
                    pass

            # Get output - may be None if redirected to file
            stdout, stderr = process.communicate()

            logger.info(f"Training process completed with exit code: {return_code}")
            if stdout:
                logger.info(f"Stdout: {stdout[:500]}...")  # Log first 500 chars
            if stderr:
                logger.error(f"Stderr: {stderr[:500]}...")  # Log first 500 chars

            # Check if job was cancelled before checking exit code
            if job_info and job_info.get("status") == "cancelled":
                return {
                    "status": "cancelled",
                    "message": "Training was cancelled by user",
                    "exit_code": return_code,
                }
            elif return_code == 0:
                return {
                    "status": "completed",
                    "message": "Training completed successfully",
                    "output": stdout or "Output redirected to log file",
                    "stdout": stdout or "Output redirected to log file",
                    "stderr": stderr if stderr else "",
                    "exit_code": return_code,
                }
            else:
                # Try to get the actual error from log file
                error_message = f"Training failed with exit code {return_code}"
                log_file_path = None

                # Try to get log file path from job_info
                if job_info and "log_file_path" in job_info:
                    log_file_path = job_info["log_file_path"]

                # Try to get actual error from log file
                if log_file_path:
                    last_error = get_last_error_from_log(log_file_path)
                    if last_error:
                        error_message = (
                            f"{last_error} (see log file for full error message)"
                        )

                return {
                    "status": "failed",
                    "error": error_message,
                    "exit_code": return_code,
                    "stdout": stdout or "Output redirected to log file",
                    "stderr": stderr or "Error output redirected to log file",
                }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def check_extraction_status(process, job_info=None):
    """Check status of running extraction process"""
    try:
        if process is None:
            return {"status": "error", "error": "No process to check"}

        # Check if process is still running
        return_code = process.poll()

        if return_code is None:
            # Process is still running
            status_response = {
                "status": "running",
                "message": "Extraction process is still running",
            }

            # Try to read detailed status from .status file
            if job_info and "job_folder" in job_info:
                status_file = os.path.join(job_info["job_folder"], ".status")
                if os.path.exists(status_file):
                    try:
                        with open(status_file, "r") as f:
                            status_data = json.load(f)
                            # Merge status file data into response
                            if "stage" in status_data:
                                status_response["stage"] = status_data["stage"]
                            if "progress" in status_data:
                                status_response["progress"] = status_data["progress"]
                            if "message" in status_data:
                                status_response["message"] = status_data["message"]
                            if "metadata" in status_data:
                                status_response["metadata"] = status_data["metadata"]
                    except Exception as e:
                        logger.debug(f"Could not read status file: {e}")

            return status_response
        else:
            # Process has completed
            # Close log file if it was opened
            if hasattr(process, "_log_file"):
                try:
                    process._log_file.close()
                except:
                    pass

            # Get output - may be None if redirected to file
            stdout, stderr = process.communicate()

            logger.info(f"Extraction process completed with exit code: {return_code}")
            if stdout:
                logger.info(f"Stdout: {stdout[:500]}...")  # Log first 500 chars
            if stderr:
                logger.error(f"Stderr: {stderr[:500]}...")  # Log first 500 chars

            # Check if job was cancelled before checking exit code
            if job_info and job_info.get("status") == "cancelled":
                return {
                    "status": "cancelled",
                    "message": "Extraction was cancelled by user",
                    "exit_code": return_code,
                }
            elif return_code == 0:
                # Try to parse extraction files from stdout if available
                extraction_files = []
                if stdout and "SUCCESS:" in stdout:
                    # Extract any created files info from stdout
                    try:
                        lines = stdout.split("\n")
                        for line in lines:
                            if "extraction_task_" in line and ".csv" in line:
                                extraction_files.append(line.strip())
                    except:
                        pass

                return {
                    "status": "completed",
                    "message": "Extraction completed successfully",
                    "output": stdout or "Output redirected to log file",
                    "stdout": stdout or "Output redirected to log file",
                    "stderr": stderr if stderr else "",
                    "exit_code": return_code,
                    "extraction_files": extraction_files,
                }
            else:
                # Try to get the actual error from log file
                error_message = f"Extraction failed with exit code {return_code}"
                log_file_path = None

                # Try to get log file path from job_info
                if job_info and "log_file_path" in job_info:
                    log_file_path = job_info["log_file_path"]

                # Try to get actual error from log file
                if log_file_path:
                    last_error = get_last_error_from_log(log_file_path)
                    if last_error:
                        error_message = (
                            f"{last_error} (see log file for full error message)"
                        )

                return {
                    "status": "failed",
                    "error": error_message,
                    "exit_code": return_code,
                    "stdout": stdout or "Output redirected to log file",
                    "stderr": stderr or "Error output redirected to log file",
                }

    except Exception as e:
        return {"status": "error", "error": str(e)}


def spec_to_image(spectrogram, range=None, colormap=None, channels=3, shape=None):
    """Convert spectrogram to image array (fast version)"""
    # Apply range if specified
    if range is not None:
        spectrogram = np.clip(spectrogram, range[0], range[1])
        spectrogram = (spectrogram - range[0]) / (range[1] - range[0])
    else:
        spec_min, spec_max = np.min(spectrogram), np.max(spectrogram)
        if spec_max > spec_min:
            spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)

    # Flip vertically (higher frequencies at top)
    spectrogram = np.flipud(spectrogram)

    # Apply colormap efficiently
    if colormap == "greys_r":
        if channels == 1:
            img_array = 1.0 - spectrogram  # Invert
        else:
            inverted = 1.0 - spectrogram
            img_array = np.stack([inverted] * 3, axis=-1)
    else:
        if channels == 1:  # greyscale
            img_array = spectrogram
        else:
            # apply matplotlib colormap
            import matplotlib.pyplot as plt

            cmap = plt.get_cmap(colormap)
            img_array = cmap(spectrogram)[:, :, :3]  # Drop alpha channel

    # Resize if shape is specified
    if shape is not None:
        from scipy.ndimage import zoom

        zoom_factors = (shape[0] / img_array.shape[0], shape[1] / img_array.shape[1])
        if len(img_array.shape) == 3:
            zoom_factors = zoom_factors + (1,)
        img_array = zoom(img_array, zoom_factors, order=1)

    # Convert to 0-255 uint8
    img_array = (img_array * 255).astype(np.uint8)
    return img_array


def process_single_clip(clip_data, settings):
    """Process a single clip with optimized performance (adapted from create_audio_clips_batch.py)"""
    try:
        file_path = clip_data["file_path"]
        start_time = clip_data["start_time"]
        end_time = clip_data["end_time"]

        # Load audio
        duration = end_time - start_time
        samples, sr = librosa.load(
            file_path, sr=None, offset=start_time, duration=duration
        )

        # Normalize audio if requested
        if settings.get("normalize_audio", True):
            samples = samples / (np.max(np.abs(samples)) + 1e-8)

        # Create spectrogram
        frequencies, _, spectrogram = scipy.signal.spectrogram(
            x=samples,
            fs=sr,
            nperseg=int(settings.get("spec_window_size", 512)),
            noverlap=int(settings.get("spec_window_size", 512) * 0.5),
            nfft=int(settings.get("spec_window_size", 512)),
        )

        # Convert to decibels
        spectrogram = 10 * np.log10(
            spectrogram,
            where=spectrogram > 0,
            out=np.full(spectrogram.shape, -np.inf),
        )

        # Apply bandpass filter if requested (frequency cropping)
        if settings.get("use_bandpass", False):
            bandpass_range = settings.get("bandpass_range", [0, 10000])
            lowest_index = np.abs(frequencies - bandpass_range[0]).argmin()
            highest_index = np.abs(frequencies - bandpass_range[1]).argmin()
            spectrogram = spectrogram[lowest_index : highest_index + 1, :]
            frequencies = frequencies[lowest_index : highest_index + 1]

        # Show reference frequency line if requested (after bandpass filtering)
        if settings.get("show_reference_frequency", False):
            ref_freq = settings.get("reference_frequency", 1000)
            # Only add reference line if frequency is within the current range
            if frequencies.min() <= ref_freq <= frequencies.max():
                closest_index = np.abs(frequencies - ref_freq).argmin()
                db_range = settings.get("dB_range", [-80, -20])
                # Make the reference line very prominent
                spectrogram[closest_index, :] = db_range[1]
                logger.info(
                    f"Added reference line at {ref_freq}Hz (index {closest_index})"
                )
            else:
                logger.warning(
                    f"Reference frequency {ref_freq}Hz is outside frequency range {frequencies.min()}-{frequencies.max()}Hz"
                )

        # Convert spectrogram to image array
        colormap = settings.get("spectrogram_colormap", "greys_r")
        img_array = spec_to_image(
            spectrogram,
            range=settings.get("dB_range", [-80, -20]),
            colormap=colormap,
            channels=1 if colormap in ["greys", "greys_r"] else 3,
            shape=(
                (settings.get("image_height", 224), settings.get("image_width", 224))
                if settings.get("resize_images", True)
                else None
            ),
        )

        # Create audio buffer (in-memory WAV)
        audio_buffer = BytesIO()
        sf.write(audio_buffer, samples, sr, format="WAV")
        audio_buffer.seek(0)
        audio_base64 = base64.b64encode(audio_buffer.read()).decode("utf-8")

        # Create spectrogram image buffer (in-memory PNG)
        img_buffer = BytesIO()

        # Convert numpy array to PIL Image for faster processing
        if len(img_array.shape) == 2:
            pil_image = Image.fromarray(img_array, mode="L")
        else:
            pil_image = Image.fromarray(img_array.astype(np.uint8))

        # Save to buffer as PNG with optimization
        pil_image.save(img_buffer, format="PNG", optimize=True, compress_level=6)
        img_buffer.seek(0)
        spectrogram_base64 = base64.b64encode(img_buffer.read()).decode("utf-8")

        return {
            "clip_id": clip_data.get("clip_id", f"{file_path}_{start_time}_{end_time}"),
            "file_path": file_path,
            "start_time": start_time,
            "end_time": end_time,
            "status": "success",
            "audio_base64": audio_base64,
            "spectrogram_base64": spectrogram_base64,
            "duration": duration,
            "sample_rate": int(sr),
            "frequency_range": [float(frequencies.min()), float(frequencies.max())],
            "time_range": [float(start_time), float(end_time)],
        }

    except Exception as e:
        logger.error(f"Error processing clip {clip_data}: {e}")
        return {
            "clip_id": clip_data.get(
                "clip_id",
                f"{clip_data.get('file_path', 'unknown')}_{clip_data.get('start_time', 0)}_{clip_data.get('end_time', 0)}",
            ),
            "file_path": clip_data.get("file_path"),
            "start_time": clip_data.get("start_time"),
            "end_time": clip_data.get("end_time"),
            "status": "error",
            "error": str(e),
        }


def _get_scripts_path():
    """
    Get the path to the scripts directory.
    Works both in development (normal Python) and when bundled with PyInstaller.
    """
    if getattr(sys, "frozen", False):
        # Running in PyInstaller bundle
        base_path = sys._MEIPASS
    else:
        # Running in normal Python
        base_path = os.path.dirname(__file__)

    return os.path.join(base_path, "scripts")


class LightweightServer:
    def __init__(self, port=8000, host="localhost"):
        self.port = port
        self.host = host
        # Increase max request body size to 100MB for large annotation files
        self.app = web.Application(client_max_size=100 * 1024 * 1024)
        self.running_jobs = (
            {}
        )  # Track running inference jobs: {job_id: {process, task, status, result}}
        self.setup_routes()
        self.setup_cors()

    def json_response_with_nan_handling(self, data, **kwargs):
        """Create JSON response with proper NaN handling"""
        import json
        import math

        def convert_nan(obj):
            if isinstance(obj, dict):
                return {key: convert_nan(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_nan(item) for item in obj]
            elif isinstance(obj, float) and math.isnan(obj):
                return None
            else:
                return obj

        clean_data = convert_nan(data)
        return web.json_response(clean_data, **kwargs)

    def setup_cors(self):
        """Setup CORS for frontend communication"""
        cors = cors_setup(
            self.app,
            defaults={
                "*": ResourceOptions(
                    allow_credentials=True,
                    expose_headers="*",
                    allow_headers="*",
                    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
                )
            },
        )

        # Add CORS to all routes
        for route in list(self.app.router.routes()):
            cors.add(route)

    def setup_routes(self):
        """Setup HTTP routes"""
        self.app.router.add_get("/", self.root_handler)
        self.app.router.add_get("/health", self.health_check)
        self.app.router.add_post("/scan_folder", self.scan_folder)
        self.app.router.add_post("/get_sample_detections", self.get_sample_detections)
        self.app.router.add_post("/load_scores", self.load_scores)
        self.app.router.add_get("/clip", self.clip_single)
        self.app.router.add_post("/clips/batch", self.clips_batch)
        self.app.router.add_delete("/cache", self.clear_cache)

        # New config and process management routes
        self.app.router.add_post("/config/save", self.save_config)
        self.app.router.add_post("/config/load", self.load_config)
        self.app.router.add_post("/config/validate", self.validate_config)
        self.app.router.add_post("/env/check", self.check_env)
        self.app.router.add_post("/env/setup", self.setup_env)
        self.app.router.add_post("/inference/run", self.run_inference)
        self.app.router.add_get("/inference/status/{job_id}", self.get_inference_status)
        self.app.router.add_post("/inference/cancel/{job_id}", self.cancel_inference)

        # Training routes
        self.app.router.add_post("/training/run", self.run_training)
        self.app.router.add_get("/training/status/{job_id}", self.get_training_status)
        self.app.router.add_post("/training/cancel/{job_id}", self.cancel_training)

        # Extraction routes
        self.app.router.add_post(
            "/extraction/scan-predictions", self.scan_predictions_folder
        )
        self.app.router.add_post("/review/load-task", self.load_review_task)
        self.app.router.add_post("/extraction/run", self.run_extraction)
        self.app.router.add_get(
            "/extraction/status/{job_id}", self.get_extraction_status
        )
        self.app.router.add_post("/extraction/cancel/{job_id}", self.cancel_extraction)

        # File counting routes
        self.app.router.add_post("/files/count-glob", self.count_files_glob)
        self.app.router.add_post("/files/count-list", self.count_files_list)
        self.app.router.add_post("/files/get-csv-columns", self.get_csv_columns)
        self.app.router.add_post("/files/count-rows", self.count_file_rows)

        # Server mode file browsing routes
        self.app.router.add_post("/files/browse", self.browse_files)
        self.app.router.add_post("/files/save", self.save_file_server)
        self.app.router.add_post("/files/read", self.read_file_server)
        self.app.router.add_post("/files/unique-name", self.generate_unique_name)

    async def root_handler(self, request):
        """Root endpoint to handle HEAD requests from wait-on"""
        return web.json_response({"status": "ok", "server": "lightweight_server"})

    async def health_check(self, request):
        """Health check endpoint"""
        return web.json_response(
            {
                "status": "ok",
                "message": f"Lightweight server running on port {self.port}",
                "port": self.port,
                "server_type": "lightweight",
                "capabilities": [
                    "scan_folder",
                    "get_sample_detections",
                    "load_scores",
                    "load_extraction_task",
                    "config_management",
                    "env_management",
                    "inference_runner",
                    "training_runner",
                    "extraction_runner",
                ],
            }
        )

    async def scan_folder(self, request):
        """Scan folder for audio files"""
        try:
            data = await request.json()
            folder_path = data.get("folder_path")

            if not folder_path or not os.path.exists(folder_path):
                raise ValueError("Invalid folder path")

            # Call scan function
            result = scan_folder.scan_folder_for_audio_files(folder_path)

            return web.json_response(result)

        except Exception as e:
            logger.error(f"Error scanning folder: {e}")
            return web.json_response({"error": str(e), "files": []}, status=500)

    async def get_sample_detections(self, request):
        """Get sample detections using lightweight approach"""
        try:
            data = await request.json()
            score_data = data.get("score_data")
            species = data.get("species")
            score_range = data.get("score_range")
            num_samples = data.get("num_samples", 12)

            # Call function
            samples = get_sample_detections.get_sample_detections(
                score_data, species, score_range, num_samples
            )

            return web.json_response(samples)

        except Exception as e:
            logger.error(f"Error getting sample detections: {e}")
            return web.json_response({"error": str(e), "samples": []}, status=500)

    async def load_scores(self, request):
        """Load scores from file"""
        try:
            data = await request.json()
            file_path = data.get("file_path")
            max_rows = data.get("max_rows")

            if not file_path or not os.path.exists(file_path):
                raise ValueError("Invalid file path")

            # Call function with optional max_rows parameter
            result = load_scores.load_scores(file_path, max_rows=max_rows)

            return self.json_response_with_nan_handling(result)

        except Exception as e:
            logger.error(f"Error loading scores: {e}")
            return web.json_response({"error": str(e), "scores": {}}, status=500)

    def _multihot_to_class_list(self, series, classes, threshold=0):
        """Helper function to convert multi-hot row to list of class names"""
        labels = series[classes]
        # Convert to numeric, treating non-numeric values as 0
        labels = pd.to_numeric(labels, errors="coerce").fillna(0)
        return labels[labels > threshold].index.to_list()

    async def load_review_task(self, request):
        """Load extraction task CSV file for the Review tab"""
        try:
            data = await request.json()
            csv_path = data.get("csv_path")
            threshold = data.get("threshold", 0)
            wide_format = data.get(
                "wide_format", False
            )  # New parameter for multi-hot format

            if not csv_path:
                return web.json_response({"error": "csv_path is required"}, status=400)

            if not os.path.exists(csv_path):
                return web.json_response(
                    {"error": f"File not found: {csv_path}"}, status=404
                )

            logger.info(f"Loading extraction task CSV: {csv_path}")

            # Read CSV file
            df = pd.read_csv(csv_path)

            # Validate required columns
            required_columns = ["file", "start_time"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                return web.json_response(
                    {
                        "error": f"Missing required columns: {', '.join(missing_columns)}"
                    },
                    status=400,
                )

            logger.info(f"Found {len(df)} clips in CSV")
            logger.info(f"Columns: {list(df.columns)}")

            # Convert numeric columns to proper types
            df["start_time"] = pd.to_numeric(df["start_time"], errors="coerce")
            if "end_time" in df.columns:
                df["end_time"] = pd.to_numeric(df["end_time"], errors="coerce")

            # Determine clip duration if end time is provided
            if (
                "end_time" in df.columns
                and len(df) > 0
                and pd.notna(df["end_time"].iloc[0])
            ):
                duration = float(df.iloc[0]["end_time"] - df.iloc[0]["start_time"])
            else:
                duration = None

            # Fill missing values
            df["id"] = list(range(len(df)))
            if "comments" in df.columns:
                df["comments"].fillna("", inplace=True)
            else:
                df["comments"] = ""

            classes = None

            # Priority: annotation column > labels column > wide format
            if "annotation" in df.columns:
                # Binary classification format
                df["annotation"].fillna("", inplace=True)
                df["annotation"] = df["annotation"].str.strip().str.lower()

                # Validate annotation values
                valid_annotations = ["yes", "no", "uncertain", ""]
                invalid = df["annotation"][~df["annotation"].isin(valid_annotations)]
                if not invalid.empty:
                    return web.json_response(
                        {
                            "error": f"annotation column contained invalid values: {invalid.unique()}. Valid values are: {valid_annotations}"
                        },
                        status=400,
                    )

                # Reorder columns
                standard_cols = ["file", "start_time"]
                if "end_time" in df.columns:
                    standard_cols.append("end_time")
                standard_cols.extend(["annotation", "comments"])

                extra_cols = [
                    col
                    for col in df.columns
                    if col not in standard_cols and col != "id"
                ]
                df = df[standard_cols + extra_cols + ["id"]]

            elif "labels" in df.columns:
                # Multi-class with labels column
                classes = set()
                df["labels"].fillna("", inplace=True)

                # Parse labels
                def parse_labels(x):
                    if pd.isna(x) or x == "":
                        return []
                    elif isinstance(x, list):
                        return x
                    elif isinstance(x, str):
                        if x.startswith("[") and x.endswith("]"):
                            try:
                                return json.loads(x.replace("'", '"'))
                            except:
                                return []
                        else:
                            return [
                                label.strip() for label in x.split(",") if label.strip()
                            ]
                    else:
                        return []

                df["labels"] = df["labels"].apply(parse_labels)

                # Extract unique classes
                for labels_list in df["labels"]:
                    if isinstance(labels_list, list):
                        classes.update(labels_list)
                classes = sorted(list(classes)) if classes else None

                # Handle annotation_status
                if "annotation_status" not in df.columns:
                    df["annotation_status"] = "unreviewed"
                else:
                    df["annotation_status"].fillna("unreviewed", inplace=True)

                # Validate annotation_status
                valid_statuses = ["complete", "unreviewed", "uncertain"]
                invalid_statuses = df["annotation_status"][
                    ~df["annotation_status"].isin(valid_statuses)
                ]
                if not invalid_statuses.empty:
                    return web.json_response(
                        {
                            "error": f"annotation_status column contained invalid values: {invalid_statuses.unique()}. Valid values are: {valid_statuses}"
                        },
                        status=400,
                    )

                # Reorder columns
                standard_cols = ["file", "start_time"]
                if "end_time" in df.columns:
                    standard_cols.append("end_time")
                standard_cols.extend(["labels", "annotation_status", "comments"])

                extra_cols = [
                    col
                    for col in df.columns
                    if col not in standard_cols and col != "id"
                ]
                df = df[standard_cols + extra_cols + ["id"]]

                # Serialize labels to JSON
                df["labels"] = df["labels"].apply(
                    lambda x: json.dumps(x) if isinstance(x, list) else "[]"
                )

            elif wide_format:
                # Multi-hot format (one column per class) - only used when explicitly requested
                classes = list(
                    set(df.columns)
                    - set(["file", "start_time", "end_time", "comments", "id"])
                )
                df["labels"] = df.apply(
                    self._multihot_to_class_list, axis=1, args=(classes, threshold)
                )

                # Serialize labels to JSON
                df["labels"] = df["labels"].apply(
                    lambda x: json.dumps(x) if isinstance(x, list) else "[]"
                )

                if "comments" not in df.columns:
                    df["comments"] = ""

                df["annotation_status"] = "unreviewed"

                # Reorder columns
                standard_cols = ["file", "start_time"]
                if "end_time" in df.columns:
                    standard_cols.append("end_time")
                standard_cols.extend(["labels", "annotation_status", "comments"])

                extra_cols = [
                    col
                    for col in df.columns
                    if col not in standard_cols and col not in classes and col != "id"
                ]
                df = df[standard_cols + extra_cols + ["id"]]

            else:
                # No annotation or labels column, and not wide format - error
                return web.json_response(
                    {
                        "error": "CSV must have either 'annotation' column (for binary review) or 'labels' column (for multiclass review). For wide-format CSV with one-hot encoded columns, use 'Open Wide-format CSV' button."
                    },
                    status=400,
                )

            # Convert to JSON
            clips = df.to_dict(orient="records")

            # Handle NaN values
            for i, clip in enumerate(clips):
                clip["id"] = i
                for key, value in clip.items():
                    if pd.isna(value):
                        clip[key] = None

            result = {
                "clips": clips,
                "total_clips": len(clips),
                "columns": list(df.columns),
                "duration": duration,
                "classes": classes,
            }

            return self.json_response_with_nan_handling(result)

        except pd.errors.EmptyDataError:
            return web.json_response({"error": "CSV file is empty"}, status=400)
        except pd.errors.ParserError as e:
            logger.error(f"CSV parse error: {e}")
            return web.json_response(
                {"error": f"Failed to parse CSV: {str(e)}"}, status=400
            )
        except Exception as e:
            logger.error(f"Error loading extraction task: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def count_file_rows(self, request):
        """Count rows in a CSV or PKL file"""
        try:
            data = await request.json()
            file_path = data.get("file_path")

            if not file_path:
                return web.json_response(
                    {"status": "error", "error": "No file_path provided"}, status=400
                )

            if not os.path.exists(file_path):
                return web.json_response(
                    {"status": "error", "error": f"File not found: {file_path}"},
                    status=400,
                )

            # Call row count function
            row_count = load_scores.count_file_rows(file_path)

            return web.json_response(
                {"status": "success", "row_count": row_count, "file_path": file_path}
            )

        except Exception as e:
            logger.error(f"Error counting file rows: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def clip_single(self, request):
        """Process single audio clip from query parameters"""
        try:
            # Get query parameters
            params = request.query

            # Extract required parameters
            file_path = params.get("file_path")
            start_time = float(params.get("start_time", 0))
            end_time = float(params.get("end_time", start_time + 3))

            if not file_path:
                return web.json_response(
                    {"error": "file_path parameter is required"}, status=400
                )

            # Convert query parameters to settings format
            settings = {
                "spec_window_size": int(params.get("spec_window_size", 512)),
                "spectrogram_colormap": params.get("spectrogram_colormap", "greys_r"),
                "dB_range": json.loads(params.get("dB_range", "[-80, -20]")),
                "use_bandpass": params.get("use_bandpass", "false").lower() == "true",
                "bandpass_range": json.loads(
                    params.get("bandpass_range", "[500, 8000]")
                ),
                "resize_images": params.get("resize_images", "true").lower() == "true",
                "image_width": int(params.get("image_width", 224)),
                "image_height": int(params.get("image_height", 224)),
                "normalize_audio": params.get("normalize_audio", "true").lower()
                == "true",
            }

            # Create clip data
            clip_data = {
                "file_path": file_path,
                "start_time": start_time,
                "end_time": end_time,
            }

            # Process the clip
            result = process_single_clip(clip_data, settings)

            if result.get("status") == "error":
                return web.json_response(result, status=500)

            return web.json_response(result)

        except Exception as e:
            logger.error(f"Error processing single clip: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def clips_batch(self, request):
        """Process batch of audio clips to generate spectrograms and audio"""
        try:
            data = await request.json()
            clips = data.get("clips", [])
            settings = data.get("settings", {})

            if not clips:
                return web.json_response({"error": "No clips provided"}, status=400)

            # Process clips using the optimized function from create_audio_clips_batch.py
            results = []
            for clip in clips:
                # Use the process_single_clip function that handles all settings properly
                result = process_single_clip(clip, settings)
                results.append(result)

            # Count successful clips
            successful_count = sum(1 for r in results if r.get("status") == "success")

            return web.json_response(
                {
                    "status": "success",
                    "results": results,
                    "successful_clips": successful_count,
                    "processing_time": 0.0,
                }
            )

        except Exception as e:
            logger.error(f"Error in clips batch processing: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def clear_cache(self, request):
        """Clear server cache (lightweight server doesn't really cache, so just return success)"""
        try:
            return web.json_response({"status": "success", "message": "Cache cleared"})
        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
            return web.json_response({"error": str(e)}, status=500)

    # Config Management Routes
    async def save_config(self, request):
        """Save inference configuration to file"""
        try:
            data = await request.json()
            config_data = data.get("config_data")
            output_path = data.get("output_path")

            if not config_data or not output_path:
                return web.json_response(
                    {"error": "config_data and output_path required"}, status=400
                )

            result = save_inference_config(config_data, output_path)
            return web.json_response(result)

        except Exception as e:
            logger.error(f"Error saving config: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def load_config(self, request):
        """Load inference configuration from file"""
        try:
            data = await request.json()
            config_path = data.get("config_path")

            if not config_path:
                return web.json_response({"error": "config_path required"}, status=400)

            result = load_inference_config(config_path)
            return web.json_response(result)

        except Exception as e:
            logger.error(f"Error loading config: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def validate_config(self, request):
        """Validate audio files in configuration"""
        try:
            data = await request.json()
            files = data.get("files", [])

            result = validate_audio_files(files)
            return web.json_response({"status": "success", "validation": result})

        except Exception as e:
            logger.error(f"Error validating config: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    # Environment Management Routes
    async def check_env(self, request):
        """Check conda-pack environment status

        If env_path not provided or None, uses default system cache directory
        """
        try:
            data = await request.json()
            env_path = data.get("env_path")  # Could be None for default

            # Use default cache directory if not provided
            if env_path is None:
                env_path = get_default_env_path()
                logger.info(f"Using default env path for check: {env_path}")
            else:
                logger.info(f"Using custom env path for check: {env_path}")

            result = check_environment(env_path)
            return web.json_response(result)

        except Exception as e:
            logger.error(f"Error checking environment: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def setup_env(self, request):
        """Setup conda-pack environment (extract if needed)

        If env_path not provided or None, uses default system cache directory and auto-downloads from Google Drive if needed.
        If env_path is provided, uses that custom environment (must already exist).
        """
        try:
            data = await request.json()
            env_path = data.get("env_path")  # None = use default cache, or custom path

            result = setup_environment(env_path)
            return web.json_response(result)

        except Exception as e:
            logger.error(f"Error setting up environment: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    # Process Management Routes
    async def run_inference(self, request):
        """Start inference process and return immediately with job ID"""
        try:
            data = await request.json()
            config_path = data.get("config_path")
            env_path = data.get("env_path")
            job_id = data.get(
                "job_id", f"job_{int(asyncio.get_event_loop().time() * 1000)}"
            )

            logger.info(f"Inference request received:")
            logger.info(f"  job_id: {job_id}")
            logger.info(f"  config_path: {config_path}")
            logger.info(f"  env_path: {env_path if env_path else 'default'}")

            if not config_path:
                return web.json_response({"error": "config_path required"}, status=400)

            # First check/setup environment (env_path can be None for default)
            env_result = setup_environment(env_path)
            if env_result["status"] != "ready":
                logger.error(f"Environment setup failed: {env_result}")
                return web.json_response(env_result, status=500)

            # Start inference process (non-blocking)
            result = start_inference_process(
                job_id, config_path, env_result["python_path"]
            )

            if result["status"] == "started":
                # Store job info for status tracking
                self.running_jobs[job_id] = {
                    "process": result["process"],
                    "status": "running",
                    "job_id": job_id,
                    "system_pid": result["system_pid"],
                    "command": result["command"],
                    "started_at": asyncio.get_event_loop().time(),
                    "log_file_path": result.get("log_file_path"),
                    "job_folder": result.get("job_folder"),
                }

                return web.json_response(
                    {
                        "status": "started",
                        "job_id": job_id,
                        "message": "Inference started successfully. Use /inference/status/{job_id} to check progress.",
                    }
                )
            else:
                return web.json_response(result, status=500)

        except Exception as e:
            logger.error(f"Error starting inference: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def get_inference_status(self, request):
        """Get status of running inference job"""
        try:
            job_id = request.match_info["job_id"]

            if job_id not in self.running_jobs:
                return web.json_response(
                    {"status": "error", "error": f"Job {job_id} not found"}, status=404
                )

            job_info = self.running_jobs[job_id]
            process = job_info["process"]

            # Check if job is already in a final state
            if job_info["status"] in ["cancelled", "completed", "failed"]:
                # Job is already finished, don't check process status again
                if job_info["status"] == "cancelled":
                    status_result = {
                        "status": "cancelled",
                        "message": "Job was cancelled by user",
                    }
                elif job_info["status"] == "completed":
                    status_result = {
                        "status": "completed",
                        "message": "Inference completed successfully",
                    }
                else:  # failed
                    status_result = {"status": "failed", "message": "Inference failed"}
            else:
                # Check current status
                status_result = check_inference_status(process, job_info)

                # Update job info
                job_info["status"] = status_result["status"]

            job_info["last_checked"] = asyncio.get_event_loop().time()

            # If completed, failed, or cancelled, add final results and optionally clean up
            if status_result["status"] in ["completed", "failed", "cancelled"]:
                job_info.update(status_result)
                # Keep job info for a while so frontend can retrieve results
                # Could add cleanup logic here if needed

            return web.json_response(
                {
                    "job_id": job_id,
                    "system_pid": job_info.get("system_pid"),
                    "started_at": job_info["started_at"],
                    "last_checked": job_info["last_checked"],
                    **status_result,
                }
            )

        except Exception as e:
            logger.error(f"Error checking inference status: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def cancel_inference(self, request):
        """Cancel a running inference job"""
        try:
            job_id = request.match_info["job_id"]

            if job_id not in self.running_jobs:
                return web.json_response(
                    {"status": "error", "error": f"Job {job_id} not found"}, status=404
                )

            job_info = self.running_jobs[job_id]
            process = job_info["process"]

            try:
                # Terminate the process
                process.terminate()
                # Give it a moment to terminate gracefully
                import time

                time.sleep(0.5)

                # If still running, force kill
                if process.poll() is None:
                    process.kill()

                # Close log file if it was opened
                if hasattr(process, "_log_file"):
                    try:
                        process._log_file.close()
                    except:
                        pass

                # Update job status
                job_info["status"] = "cancelled"
                job_info["cancelled_at"] = asyncio.get_event_loop().time()

                logger.info(f"Inference job {job_id} cancelled successfully")

                return web.json_response(
                    {
                        "status": "cancelled",
                        "job_id": job_id,
                        "message": "Inference job cancelled successfully",
                    }
                )

            except Exception as e:
                logger.error(f"Error cancelling inference job {job_id}: {e}")
                return web.json_response(
                    {"status": "error", "error": f"Failed to cancel job: {str(e)}"},
                    status=500,
                )

        except Exception as e:
            logger.error(f"Error in cancel_inference: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    # Training Process Management Routes
    async def run_training(self, request):
        """Start training process and return immediately with job ID"""
        try:
            data = await request.json()
            config_path = data.get("config_path")
            env_path = data.get("env_path")
            job_id = data.get(
                "job_id", f"training_job_{int(asyncio.get_event_loop().time() * 1000)}"
            )

            logger.info(f"Training request received:")
            logger.info(f"  job_id: {job_id}")
            logger.info(f"  config_path: {config_path}")
            logger.info(f"  env_path: {env_path if env_path else 'default'}")

            if not config_path:
                return web.json_response({"error": "config_path required"}, status=400)

            # First check/setup environment (env_path can be None for default)
            env_result = setup_environment(env_path)
            if env_result["status"] != "ready":
                logger.error(f"Environment setup failed: {env_result}")
                return web.json_response(env_result, status=500)

            # Start training process (non-blocking)
            result = start_training_process(
                job_id, config_path, env_result["python_path"]
            )

            if result["status"] == "started":
                # Store job info for status tracking
                self.running_jobs[job_id] = {
                    "process": result["process"],
                    "status": "running",
                    "job_id": job_id,
                    "system_pid": result["system_pid"],
                    "command": result["command"],
                    "started_at": asyncio.get_event_loop().time(),
                    "job_type": "training",
                    "log_file_path": result.get("log_file_path"),
                    "job_folder": result.get("job_folder"),
                }

                return web.json_response(
                    {
                        "status": "started",
                        "job_id": job_id,
                        "message": "Training started successfully. Use /training/status/{job_id} to check progress.",
                    }
                )
            else:
                return web.json_response(result, status=500)

        except Exception as e:
            logger.error(f"Error starting training: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def get_training_status(self, request):
        """Get status of running training job"""
        try:
            job_id = request.match_info["job_id"]

            if job_id not in self.running_jobs:
                return web.json_response(
                    {"status": "error", "error": f"Training job {job_id} not found"},
                    status=404,
                )

            job_info = self.running_jobs[job_id]
            process = job_info["process"]

            # Check if job is already in a final state
            if job_info["status"] in ["cancelled", "completed", "failed"]:
                # Job is already finished, don't check process status again
                if job_info["status"] == "cancelled":
                    status_result = {
                        "status": "cancelled",
                        "message": "Job was cancelled by user",
                    }
                elif job_info["status"] == "completed":
                    status_result = {
                        "status": "completed",
                        "message": "Training completed successfully",
                    }
                else:  # failed
                    status_result = {"status": "failed", "message": "Training failed"}
            else:
                # Check current status
                status_result = check_training_status(process, job_info)

                # Update job info
                job_info["status"] = status_result["status"]

            job_info["last_checked"] = asyncio.get_event_loop().time()

            # If completed, failed, or cancelled, add final results and optionally clean up
            if status_result["status"] in ["completed", "failed", "cancelled"]:
                job_info.update(status_result)
                # Keep job info for a while so frontend can retrieve results
                # Could add cleanup logic here if needed

            return web.json_response(
                {
                    "job_id": job_id,
                    "system_pid": job_info.get("system_pid"),
                    "job_type": "training",
                    "started_at": job_info["started_at"],
                    "last_checked": job_info["last_checked"],
                    **status_result,
                }
            )

        except Exception as e:
            logger.error(f"Error checking training status: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def cancel_training(self, request):
        """Cancel a running training job"""
        try:
            job_id = request.match_info["job_id"]

            if job_id not in self.running_jobs:
                return web.json_response(
                    {"status": "error", "error": f"Training job {job_id} not found"},
                    status=404,
                )

            job_info = self.running_jobs[job_id]
            process = job_info["process"]

            try:
                # Terminate the process
                process.terminate()
                # Give it a moment to terminate gracefully
                import time

                time.sleep(0.5)

                # If still running, force kill
                if process.poll() is None:
                    process.kill()

                # Close log file if it was opened
                if hasattr(process, "_log_file"):
                    try:
                        process._log_file.close()
                    except:
                        pass

                # Update job status
                job_info["status"] = "cancelled"
                job_info["cancelled_at"] = asyncio.get_event_loop().time()

                logger.info(f"Training job {job_id} cancelled successfully")

                return web.json_response(
                    {
                        "status": "cancelled",
                        "job_id": job_id,
                        "message": "Training job cancelled successfully",
                    }
                )

            except Exception as e:
                logger.error(f"Error cancelling training job {job_id}: {e}")
                return web.json_response(
                    {"status": "error", "error": f"Failed to cancel job: {str(e)}"},
                    status=500,
                )

        except Exception as e:
            logger.error(f"Error in cancel_training: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    # Annotation Process Management Routes
    async def scan_predictions_folder(self, request):
        """Scan folder for prediction files and extract available classes"""
        try:
            data = await request.json()
            folder_path = data.get("folder_path")

            if not folder_path:
                return web.json_response(
                    {"error": "folder_path is required"}, status=400
                )

            # Call the scan function from the extraction script
            result = clip_extraction.scan_predictions_folder(folder_path)

            return web.json_response({"status": "success", **result})

        except Exception as e:
            logger.error(f"Error scanning predictions folder: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def run_extraction(self, request):
        """Start extraction process and return immediately with job ID"""
        try:
            data = await request.json()
            config_path = data.get("config_path")
            env_path = data.get("env_path")
            job_id = data.get(
                "job_id",
                f"extraction_job_{int(asyncio.get_event_loop().time() * 1000)}",
            )

            logger.info(f"Annotation request received:")
            logger.info(f"  job_id: {job_id}")
            logger.info(f"  config_path: {config_path}")
            logger.info(f"  env_path: {env_path if env_path else 'default'}")

            if not config_path:
                return web.json_response({"error": "config_path required"}, status=400)

            # First check/setup environment (env_path can be None for default)
            env_result = setup_environment(env_path)
            if env_result["status"] != "ready":
                logger.error(f"Environment setup failed: {env_result}")
                return web.json_response(env_result, status=500)

            # Start extraction process (non-blocking)
            result = start_extraction_process(
                job_id, config_path, env_result["python_path"]
            )

            if result["status"] == "started":
                # Store job info for status tracking
                self.running_jobs[job_id] = {
                    "process": result["process"],
                    "status": "running",
                    "job_id": job_id,
                    "system_pid": result["system_pid"],
                    "command": result["command"],
                    "started_at": asyncio.get_event_loop().time(),
                    "job_type": "extraction",
                    "log_file_path": result.get("log_file_path"),
                    "job_folder": result.get("job_folder"),
                }

                return web.json_response(
                    {
                        "status": "started",
                        "job_id": job_id,
                        "message": "Extraction started successfully. Use /extraction/status/{job_id} to check progress.",
                    }
                )
            else:
                return web.json_response(result, status=500)

        except Exception as e:
            logger.error(f"Error starting extraction: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def get_extraction_status(self, request):
        """Get status of running extraction job"""
        try:
            job_id = request.match_info["job_id"]

            if job_id not in self.running_jobs:
                return web.json_response(
                    {"status": "error", "error": f"Annotation job {job_id} not found"},
                    status=404,
                )

            job_info = self.running_jobs[job_id]
            process = job_info["process"]

            # Check if job is already in a final state
            if job_info["status"] in ["cancelled", "completed", "failed"]:
                # Job is already finished, don't check process status again
                if job_info["status"] == "cancelled":
                    status_result = {
                        "status": "cancelled",
                        "message": "Job was cancelled by user",
                    }
                elif job_info["status"] == "completed":
                    status_result = {
                        "status": "completed",
                        "message": "Annotation completed successfully",
                        "extraction_files": job_info.get("extraction_files", []),
                    }
                else:  # failed
                    status_result = {"status": "failed", "message": "Annotation failed"}
            else:
                # Check current status
                status_result = check_extraction_status(process, job_info)

                # Update job info
                job_info["status"] = status_result["status"]

                # Store extraction files if completed
                if (
                    status_result["status"] == "completed"
                    and "extraction_files" in status_result
                ):
                    job_info["extraction_files"] = status_result["extraction_files"]

            job_info["last_checked"] = asyncio.get_event_loop().time()

            # If completed, failed, or cancelled, add final results and optionally clean up
            if status_result["status"] in ["completed", "failed", "cancelled"]:
                job_info.update(status_result)
                # Keep job info for a while so frontend can retrieve results
                # Could add cleanup logic here if needed

            return web.json_response(
                {
                    "job_id": job_id,
                    "system_pid": job_info.get("system_pid"),
                    "job_type": "extraction",
                    "started_at": job_info["started_at"],
                    "last_checked": job_info["last_checked"],
                    **status_result,
                }
            )

        except Exception as e:
            logger.error(f"Error checking extraction status: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def cancel_extraction(self, request):
        """Cancel a running extraction job"""
        try:
            job_id = request.match_info["job_id"]

            if job_id not in self.running_jobs:
                return web.json_response(
                    {"status": "error", "error": f"Annotation job {job_id} not found"},
                    status=404,
                )

            job_info = self.running_jobs[job_id]
            process = job_info["process"]

            try:
                # Terminate the process
                process.terminate()
                # Give it a moment to terminate gracefully
                import time

                time.sleep(0.5)

                # If still running, force kill
                if process.poll() is None:
                    process.kill()

                # Close log file if it was opened
                if hasattr(process, "_log_file"):
                    try:
                        process._log_file.close()
                    except:
                        pass

                # Update job status
                job_info["status"] = "cancelled"
                job_info["cancelled_at"] = asyncio.get_event_loop().time()

                logger.info(f"Annotation job {job_id} cancelled successfully")

                return web.json_response(
                    {
                        "status": "cancelled",
                        "job_id": job_id,
                        "message": "Annotation job cancelled successfully",
                    }
                )

            except Exception as e:
                logger.error(f"Error cancelling extraction job {job_id}: {e}")
                return web.json_response(
                    {"status": "error", "error": f"Failed to cancel job: {str(e)}"},
                    status=500,
                )

        except Exception as e:
            logger.error(f"Error in cancel_extraction: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def count_files_glob(self, request):
        """Count files matching glob patterns"""
        try:
            data = await request.json()
            patterns = data.get("patterns", [])
            extensions = data.get(
                "extensions", ["wav", "mp3", "flac", "ogg", "m4a", "aac"]
            )  # Default extensions

            if not patterns:
                return web.json_response(
                    {"status": "error", "error": "No patterns provided"}, status=400
                )

            logger.info(f"Counting files for patterns: {patterns}")
            logger.info(f"Using extensions: {extensions}")

            # Convert extensions to valid set (both lower and uppercase)
            valid_extensions = set()
            for ext in extensions:
                valid_extensions.add(f".{ext.lower()}")
                valid_extensions.add(f".{ext.upper()}")

            all_files = set()  # Use set to avoid duplicates

            for pattern in patterns:
                try:
                    # Use glob with recursive=True to support ** syntax
                    matches = glob.glob(pattern, recursive=True)

                    # Filter for audio files only
                    audio_files = [
                        f
                        for f in matches
                        if Path(f).suffix in valid_extensions and os.path.isfile(f)
                    ]
                    all_files.update(audio_files)

                    logger.info(
                        f"Pattern '{pattern}' matched {len(audio_files)} audio files"
                    )

                except Exception as e:
                    logger.warning(f"Error processing pattern '{pattern}': {e}")
                    continue

            # Convert to sorted list to get consistent first file
            all_files_list = sorted(list(all_files))
            total_count = len(all_files_list)
            first_file = all_files_list[0] if all_files_list else None

            logger.info(f"Total unique audio files found: {total_count}")
            if first_file:
                logger.info(f"First file: {first_file}")

            return web.json_response(
                {
                    "status": "success",
                    "count": total_count,
                    "first_file": first_file,
                    "patterns_processed": len(patterns),
                    "extensions_used": extensions,
                }
            )

        except Exception as e:
            logger.error(f"Error counting files from glob patterns: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def count_files_list(self, request):
        """Count files from a file list (one file per line)"""
        try:
            data = await request.json()
            file_path = data.get("file_path")

            if not file_path:
                return web.json_response(
                    {"status": "error", "error": "No file_path provided"}, status=400
                )

            if not os.path.exists(file_path):
                return web.json_response(
                    {"status": "error", "error": f"File list not found: {file_path}"},
                    status=400,
                )

            logger.info(f"Counting files from list: {file_path}")

            # Valid audio extensions
            valid_extensions = {
                ".wav",
                ".WAV",
                ".mp3",
                ".MP3",
                ".flac",
                ".FLAC",
                ".ogg",
                ".OGG",
                ".m4a",
                ".M4A",
                ".aac",
                ".AAC",
            }

            valid_files = []
            invalid_files = []

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        file_path_line = line.strip()

                        # Skip empty lines
                        if not file_path_line:
                            continue

                        # Check if file exists and has valid extension
                        if (
                            os.path.isfile(file_path_line)
                            and Path(file_path_line).suffix in valid_extensions
                        ):
                            valid_files.append(file_path_line)
                        else:
                            invalid_files.append(f"Line {line_num}: {file_path_line}")
                            logger.warning(
                                f"Invalid or missing file at line {line_num}: {file_path_line}"
                            )

                first_file = valid_files[0] if valid_files else None

                logger.info(
                    f"File list processed: {len(valid_files)} valid, {len(invalid_files)} invalid"
                )
                if first_file:
                    logger.info(f"First file: {first_file}")

                return web.json_response(
                    {
                        "status": "success",
                        "count": len(valid_files),
                        "first_file": first_file,
                        "valid_files": len(valid_files),
                        "invalid_files": len(invalid_files),
                    }
                )

            except UnicodeDecodeError:
                return web.json_response(
                    {
                        "status": "error",
                        "error": "File list contains invalid characters (not UTF-8)",
                    },
                    status=400,
                )

        except Exception as e:
            logger.error(f"Error counting files from file list: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def get_csv_columns(self, request):
        """Get column names from a CSV or PKL file"""
        try:
            data = await request.json()
            file_path = data.get("file_path")

            if not file_path:
                return web.json_response(
                    {"status": "error", "error": "No file_path provided"}, status=400
                )

            if not os.path.exists(file_path):
                return web.json_response(
                    {
                        "status": "error",
                        "error": f"Predictions file not found: {file_path}",
                    },
                    status=400,
                )

            file_ext = os.path.splitext(file_path)[1].lower()
            logger.info(f"Reading columns from {file_ext} file: {file_path}")

            try:
                if file_ext == ".pkl":
                    # Read pickle file and get column names
                    df = pd.read_pickle(file_path)
                    columns = df.columns.tolist()
                    logger.info(f"PKL columns: {columns}")
                else:
                    # Read just the header row to get column names for CSV
                    df = pd.read_csv(file_path, nrows=0)
                    columns = df.columns.tolist()
                    logger.info(f"CSV columns: {columns}")

                return web.json_response(
                    {
                        "status": "success",
                        "columns": columns,
                        "file_path": file_path,
                        "file_type": file_ext,
                    }
                )

            except Exception as e:
                return web.json_response(
                    {
                        "status": "error",
                        "error": f"Failed to read predictions file: {str(e)}",
                    },
                    status=400,
                )

        except Exception as e:
            logger.error(f"Error getting file columns: {e}")
            return web.json_response({"status": "error", "error": str(e)}, status=500)

    async def browse_files(self, request):
        """Browse server-side files (server mode only)"""
        try:
            data = await request.json()
            path = data.get("path", os.path.expanduser("~"))

            # Security: Restrict to allowed base paths
            # Allow user home directory and common data directories
            allowed_paths = [
                os.path.expanduser("~"),
                "/data",
                "/mnt",
                "/media",
                "/Users",  # macOS
                "/home",  # Linux
            ]

            # Check if path is under any allowed directory
            normalized_path = os.path.normpath(os.path.abspath(path))
            is_allowed = any(
                normalized_path.startswith(os.path.normpath(os.path.abspath(base)))
                for base in allowed_paths
            )

            if not is_allowed:
                return web.json_response(
                    {"error": "Access denied to this directory"}, status=403
                )

            # Check if path exists
            if not os.path.exists(normalized_path):
                return web.json_response(
                    {"error": f"Path does not exist: {path}"}, status=404
                )

            # List directory contents
            items = []
            try:
                for entry in os.scandir(normalized_path):
                    try:
                        stat_info = entry.stat()
                        items.append(
                            {
                                "id": entry.path,
                                "value": entry.name,
                                "size": stat_info.st_size if entry.is_file() else 0,
                                "date": int(stat_info.st_mtime * 1000),  # milliseconds
                                "type": "folder" if entry.is_dir() else "file",
                            }
                        )
                    except (OSError, PermissionError):
                        # Skip files/folders we can't access
                        continue

                # Sort: folders first, then by name
                items.sort(key=lambda x: (x["type"] != "folder", x["value"].lower()))

            except PermissionError:
                return web.json_response({"error": "Permission denied"}, status=403)

            return web.json_response({"data": items, "path": normalized_path})

        except Exception as e:
            logger.error(f"Error browsing files: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def save_file_server(self, request):
        """Save file on server (server mode only)"""
        try:
            data = await request.json()
            file_path = data.get("path")
            content = data.get("content")

            if not file_path or content is None:
                return web.json_response(
                    {"error": "Missing path or content"}, status=400
                )

            # Security: Validate path is under allowed directories
            allowed_paths = [
                os.path.expanduser("~"),
                "/data",
                "/mnt",
                "/media",
                "/Users",
                "/home",
            ]

            normalized_path = os.path.normpath(os.path.abspath(file_path))
            is_allowed = any(
                normalized_path.startswith(os.path.normpath(os.path.abspath(base)))
                for base in allowed_paths
            )

            if not is_allowed:
                return web.json_response(
                    {"error": "Access denied to this location"}, status=403
                )

            # Create parent directories if needed
            parent_dir = os.path.dirname(normalized_path)
            if parent_dir:  # Only create if there's a parent directory
                os.makedirs(parent_dir, exist_ok=True)

            # Write file
            with open(normalized_path, "w", encoding="utf-8") as f:
                f.write(content)

            return web.json_response({"status": "success", "path": normalized_path})

        except Exception as e:
            import traceback

            logger.error(f"Error saving file to {file_path}: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return web.json_response({"error": str(e)}, status=500)

    async def read_file_server(self, request):
        """Read file content (server mode only)"""
        try:
            data = await request.json()
            file_path = data.get("file_path")

            if not file_path:
                return web.json_response({"error": "Missing file_path"}, status=400)

            # Security: Validate path is under allowed directories
            allowed_paths = [
                os.path.expanduser("~"),
                "/data",
                "/mnt",
                "/media",
                "/Users",
                "/home",
            ]

            normalized_path = os.path.normpath(os.path.abspath(file_path))
            is_allowed = any(
                normalized_path.startswith(os.path.normpath(os.path.abspath(base)))
                for base in allowed_paths
            )

            if not is_allowed:
                return web.json_response(
                    {"error": "Access denied to this location"}, status=403
                )

            # Check if file exists
            if not os.path.exists(normalized_path):
                return web.json_response(
                    {"error": f"File does not exist: {file_path}"}, status=404
                )

            # Read file content
            with open(normalized_path, "r", encoding="utf-8") as f:
                content = f.read()

            return web.json_response({"content": content})

        except Exception as e:
            logger.error(f"Error reading file: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def generate_unique_name(self, request):
        """Generate unique folder/file name (server mode only)"""
        try:
            data = await request.json()
            base_path = data.get("basePath")
            folder_name = data.get("folderName")

            if not base_path or not folder_name:
                return web.json_response(
                    {"error": "Missing basePath or folderName"}, status=400
                )

            # Check if base path exists
            if not os.path.exists(base_path):
                return web.json_response(
                    {"error": f"Base path does not exist: {base_path}"}, status=404
                )

            # Generate unique name
            unique_name = folder_name
            counter = 1

            while os.path.exists(os.path.join(base_path, unique_name)):
                unique_name = f"{folder_name}_{counter}"
                counter += 1

            return web.json_response({"uniqueName": unique_name})

        except Exception as e:
            logger.error(f"Error generating unique name: {e}")
            return web.json_response({"error": str(e)}, status=500)

    async def start_server(self):
        """Start the HTTP server"""
        runner = web.AppRunner(self.app)
        await runner.setup()

        site = web.TCPSite(runner, self.host, self.port)
        await site.start()

        logger.info(f"Lightweight server started on http://{self.host}:{self.port}")
        return runner


def main():
    parser = argparse.ArgumentParser(description="Lightweight bioacoustics server")
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Port to run on (overridden by config file if provided)",
    )
    parser.add_argument(
        "--host",
        type=str,
        default="localhost",
        help="Host to bind to (use 0.0.0.0 for server mode, overridden by config file if provided)",
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to YAML config file (server.host and server.port will override command-line args)",
    )
    parser.add_argument(
        "--parent-pid",
        type=int,
        default=None,
        help="Parent process PID for heartbeat monitoring",
    )
    parser.add_argument("--test", action="store_true", help="Run quick test and exit")

    args = parser.parse_args()

    # Read config file if provided
    config = {}
    if args.config:
        try:
            with open(args.config, "r") as f:
                config = yaml.safe_load(f)
            logger.info(f"Loaded config file: {args.config}")
        except Exception as e:
            logger.error(f"Failed to load config file {args.config}: {e}")
            sys.exit(1)

    # Use config file values if available, otherwise fall back to command-line args
    host = args.host
    port = args.port

    if config:
        if "server" in config:
            if "host" in config["server"]:
                host = config["server"]["host"]
                logger.info(f"Using host from config file: {host}")
            if "port" in config["server"]:
                port = config["server"]["port"]
                logger.info(f"Using port from config file: {port}")

    logger.info(f"Server will start on {host}:{port}")

    if args.test:
        print(" Lightweight server test successful!")
        print(f"Python version: {sys.version}")
        print(
            f"PyInstaller bundling: {' SUCCESS' if getattr(sys, 'frozen', False) else ' Not bundled'}"
        )
        print("Available libraries:")
        print("  - pandas ")
        print("  - numpy ")
        print("  - librosa ")
        print("  - PIL ")
        print("  - aiohttp ")
        return 0

    async def run_server():
        server = LightweightServer(port=port, host=host)
        runner = await server.start_server()

        # Get parent process ID for heartbeat monitoring
        # Use provided parent PID if available, otherwise use getppid()
        parent_pid = args.parent_pid if args.parent_pid else os.getppid()
        logger.info(
            f"Server started with parent process PID: {parent_pid} (provided={args.parent_pid is not None})"
        )
        logger.info(f"Server process PID: {os.getpid()}")

        # Create shutdown event for graceful termination
        shutdown_event = asyncio.Event()

        async def graceful_shutdown():
            """Callback for graceful shutdown when parent dies"""
            logger.info("Graceful shutdown: setting shutdown event")
            shutdown_event.set()
            logger.info("Graceful shutdown: cleaning up runner")
            try:
                await runner.cleanup()
                logger.info("Graceful shutdown: runner cleanup completed")
            except Exception as e:
                logger.error(f"Error during runner cleanup: {e}")

        # Start parent process monitoring task
        monitor_task = asyncio.create_task(
            monitor_parent_process(parent_pid, graceful_shutdown, check_interval=2.0)
        )

        try:
            # Keep server running until shutdown event is triggered
            await shutdown_event.wait()
            logger.info("Shutdown event received, server stopped")
            logger.info("Exiting server process...")
        except KeyboardInterrupt:
            logger.info("Keyboard interrupt received, shutting down server...")
            await runner.cleanup()
        except Exception as e:
            logger.error(f"Unexpected error in server main loop: {e}")
            await runner.cleanup()
        finally:
            # Cancel monitoring task if still running
            if not monitor_task.done():
                monitor_task.cancel()
                try:
                    await monitor_task
                except asyncio.CancelledError:
                    pass

    try:
        asyncio.run(run_server())
    except Exception as e:
        logger.error(f"Error running server: {e}")
    finally:
        logger.info("Server main() function exiting")
        # Force exit to ensure no lingering tasks
        sys.exit(0)


if __name__ == "__main__":
    sys.exit(main())
